{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Parallelization Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribute works to different processes\n",
    "I follow the official documentation [julia parallel computing](http://docs.julialang.org/en/latest/manual/parallel-computing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function retuns the (irange,jrange) indexes assigned to this worker\n",
    "@everywhere function myrange(q::SharedArray)\n",
    "    idx = indexpids(q)\n",
    "    if idx == 0\n",
    "        # This worker is not assigned a piece\n",
    "        return 1:0, 1:0\n",
    "    end\n",
    "    nchunks = length(procs(q))\n",
    "    splits = [round(Int, s) for s in linspace(0,size(q,2),nchunks+1)]\n",
    "    1:size(q,1), splits[idx]+1:splits[idx+1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding two matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's the kernel\n",
    "@everywhere function advection_chunk!(q, u, irange, jrange)\n",
    "   # @show (irange, jrange)  # display so we can see what's happening\n",
    "    for j in jrange, i in irange\n",
    "        q[i,j] = q[i,j] +  u[i,j]\n",
    "    end\n",
    "    q\n",
    "end\n",
    "\n",
    "\n",
    "# Here's a convenience wrapper for a SharedArray implementation\n",
    "@everywhere advection_shared_chunk!(q, u) = advection_chunk!(q, u, myrange(q)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial code (without parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advection_serial! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advection_serial!(q, u) = advection_chunk!(q, u, 1:size(q,1), 1:size(q,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advection_shared! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function advection_shared!(q, u)\n",
    "    @sync begin\n",
    "        for p in procs(q)            \n",
    "            @time    @async remotecall_wait(advection_shared_chunk!, p, q, u)\n",
    "        end \n",
    "        end\n",
    "    q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(irange,jrange) = (1:5,1:10000)\n",
      "\tFrom worker 3:\t(irange,jrange) = (1:5,2501:5000)\n",
      "\tFrom worker 2:\t(irange,jrange) = (1:5,1:2500)\n",
      "\tFrom worker 5:\t(irange,jrange) = (1:5,7501:10000)\n",
      "\tFrom worker 4:\t(irange,jrange) = (1:5,5001:7500)\n"
     ]
    }
   ],
   "source": [
    "q = SharedArray(Float64, (5,10000))\n",
    "u = SharedArray(Float64, (5,10000))\n",
    "advection_serial!(q,u);\n",
    "advection_shared!(q,u);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000243 seconds (4 allocations: 160 bytes)\n"
     ]
    }
   ],
   "source": [
    "@time advection_serial!(q,u);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001272 seconds (2.59 k allocations: 198.359 KB)\n"
     ]
    }
   ],
   "source": [
    "@time advection_shared!(q,u);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulated serial code is faster than parallel code. What is going on???\n",
    "\n",
    "It seems that there is a lot of overhead to parallel computation. Unless each worker is doing a fairly significant amount of work, speedup is not achieved via parallelization.\n",
    "\n",
    "To see whether it is the case, I time each work done by each worker. From the total time, I can subtract these times to get time related to overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000005 seconds (6 allocations: 928 bytes)\n",
      "  0.000004 seconds (5 allocations: 880 bytes)\n",
      "  0.000002 seconds (5 allocations: 880 bytes)\n",
      "  0.000002 seconds (5 allocations: 880 bytes)\n",
      "  0.003274 seconds (3.25 k allocations: 229.750 KB)\n"
     ]
    }
   ],
   "source": [
    "@time advection_shared!(q,u);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The table below shows the results from experimenting with number of workers and number of grid points.\n",
    "\n",
    "| workers |  grid points  |  serial time   |  parallel time |  Ratio   |\n",
    "|---------|---------------|----------------|----------------|----------|\n",
    "|   3     |     10000     |    0.000224    |    0.001189    |   5.30   |\n",
    "|         |    100000     |    0.001433    |    0.001778    |   1.2    |\n",
    "|         |    200000     |    0.003179    |    0.002934    |   0.92   |\n",
    "|   5     |     10000     |    0.000228    |    0.001595    |   6.99   |\n",
    "|         |    100000     |    0.001636    |    0.002169    |   1.32   |\n",
    "|         |    200000     |    0.003357    |    0.002605    |   0.77   |\n",
    "|   7     |     10000     |    0.000188    |    0.002354    |  12.52   |\n",
    "|         |    100000     |    0.001403    |    0.002680    |   1.91   |\n",
    "|         |    200000     |    0.003055    |    0.004268    |   1.39   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
